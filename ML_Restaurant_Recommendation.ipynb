{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Gh86ualenp"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy geopy nltk vaderSentiment scikit-learn scipy flask flask-cors pyngrok\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQvk5ANaqGge"
      },
      "outputs": [],
      "source": [
        "!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n",
        "\t| sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null \\\n",
        "\t&& echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n",
        "\t| sudo tee /etc/apt/sources.list.d/ngrok.list \\\n",
        "\t&& sudo apt update \\\n",
        "\t&& sudo apt install ngrok\n",
        "\n",
        "!ngrok config add-authtoken YOUR_AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ot6DA_IjhSf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import gradio\n",
        "\n",
        "\n",
        "# üîπ Google API Key\n",
        "API_KEY = \"\"  # Replace with your actual key\n",
        "\n",
        "# üîπ Base URLs for Google Places API v2\n",
        "PLACES_SEARCH_URL = \"https://places.googleapis.com/v1/places:searchNearby\"\n",
        "PLACES_DETAILS_URL = \"https://places.googleapis.com/v1/places/\"\n",
        "GEOCODE_API_URL = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "\n",
        "# Expanded List: Comprehensive Cuisine Keywords & Variants with Afghan Cuisine and 10 Keywords Each\n",
        "CUISINE_KEYWORDS = {\n",
        "    \"arabic\": [\"arabic\", \"middle_eastern\", \"levantine\", \"khaleeji\", \"gulf_cuisine\",\n",
        "               \"mezze\", \"manakish\", \"hummus\", \"falafel\", \"fattoush\"],\n",
        "    \"saudi\": [\"saudi\", \"saudi_arabian\", \"najdi\", \"hijazi\", \"bedouin_food\",\n",
        "              \"kabsa\", \"jareesh\", \"mutabbaq\", \"saleeg\", \"mofatah\"],\n",
        "    \"yemeni\": [\"yemeni\", \"mandi\", \"madfoon\", \"zurbian\", \"haneeth\",\n",
        "               \"saltah\", \"fahsa\", \"lahsa\", \"aseeda\", \"maraq\"],\n",
        "    \"lebanese\": [\"lebanese\", \"lebanon_food\", \"shawarma_house\", \"mezze\",\n",
        "                 \"tabbouleh\", \"kibbeh\", \"fattoush\", \"hummus\", \"batata_harra\"],\n",
        "    \"turkish\": [\"turkish\", \"turkey_food\", \"ottoman_cuisine\", \"doner_kebab\",\n",
        "                \"pide\", \"lahmacun\", \"baklava\", \"kebab\", \"manti\"],\n",
        "    \"indian\": [\"indian\", \"biryani_house\", \"tandoori\", \"mughlai\", \"chicken_tikka\",\n",
        "               \"masala_dosa\", \"dal_makhani\", \"paneer_butter\", \"samosa\", \"naan\"],\n",
        "    \"pakistani\": [\"pakistani\", \"karahi\", \"nihari\", \"haleem\", \"punjabi_cuisine\",\n",
        "                  \"chicken_sajji\", \"chapli_kebab\", \"bhuna_gosht\", \"korma\", \"paya\"],\n",
        "    \"afghan\": [\"afghan\", \"kabuli_pulao\", \"afghani_kebab\", \"afghanistan_food\",\n",
        "               \"bolani\", \"ashak\", \"mantoo\", \"shorwa\", \"chalow\", \"qabeli\"],\n",
        "    \"chinese\": [\"chinese\", \"szechuan\", \"dim_sum\", \"hong_kong_food\", \"hot_pot\",\n",
        "                \"kung_pao\", \"mapo_tofu\", \"peking_duck\", \"wonton\", \"spring_rolls\"],\n",
        "    \"japanese\": [\"japanese\", \"sushi\", \"ramen\", \"yakitori\", \"izakaya\",\n",
        "                 \"tempura\", \"udon\", \"miso_soup\", \"teriyaki\", \"bento\"],\n",
        "    \"korean\": [\"korean\", \"kimchi\", \"bibimbap\", \"kbbq\", \"korean_bbq\",\n",
        "               \"bulgogi\", \"tteokbokki\", \"samgyeopsal\", \"soondubu\", \"japchae\"],\n",
        "    \"thai\": [\"thai\", \"pad_thai\", \"tom_yum\", \"green_curry\", \"som_tum\",\n",
        "             \"massaman_curry\", \"larb\", \"satay\", \"kao_pad\", \"yam_talay\"],\n",
        "    \"malaysian\": [\"malaysian\", \"nasi_lemak\", \"rendang\", \"mee_goreng\", \"char_kway_teow\",\n",
        "                  \"laksa\", \"roti_canai\", \"hainanese_chicken\", \"satay\", \"cendol\"],\n",
        "    \"italian\": [\"italian\", \"pizza\", \"pasta\", \"ristorante\", \"italian_trattoria\",\n",
        "                \"lasagna\", \"risotto\", \"bruschetta\", \"osso_buco\", \"caprese\"],\n",
        "    \"french\": [\"french\", \"baguette\", \"croissant\", \"bistro\", \"p√¢tisserie\",\n",
        "               \"ratatouille\", \"coq_au_vin\", \"boeuf_bourguignon\", \"souffl√©\", \"escargot\"],\n",
        "    \"greek\": [\"greek\", \"gyro\", \"tzatziki\", \"mediterranean_food\", \"moussaka\",\n",
        "              \"souvlaki\", \"spanakopita\", \"dolmades\", \"baklava\", \"horta\"],\n",
        "    \"spanish\": [\"spanish\", \"paella\", \"tapas\", \"churros\", \"gazpacho\",\n",
        "                \"patatas_bravas\", \"fabada\", \"croquetas\", \"jamon\", \"flan\"],\n",
        "    \"german\": [\"german\", \"bratwurst\", \"bavarian_food\", \"schnitzel\", \"pretzel\",\n",
        "               \"kartoffelsalat\", \"eintopf\", \"lebkuchen\", \"currywurst\", \"apfelstrudel\"],\n",
        "    \"american\": [\"american\", \"burgers\", \"steakhouse\", \"fried_chicken\", \"barbeque\",\n",
        "                 \"mac_and_cheese\", \"hot_dog\", \"bbq_ribs\", \"buffalo_wings\", \"meatloaf\"],\n",
        "    \"mexican\": [\"mexican\", \"tacos\", \"burritos\", \"quesadillas\", \"salsa\",\n",
        "                \"tamales\", \"chiles_rellenos\", \"pozole\", \"enchiladas\", \"guacamole\"],\n",
        "    \"ethiopian\": [\"ethiopian\", \"injera\", \"doro_wat\", \"habesha_food\", \"tibs\",\n",
        "                  \"kitfo\", \"shiro\", \"berbere\", \"foul\", \"gomen\"],\n",
        "    \"nigerian\": [\"nigerian\", \"jollof_rice\", \"suya\", \"pounded_yam\", \"egusi_soup\",\n",
        "                 \"fufu\", \"moimoi\", \"okro_soup\", \"afang_soup\", \"banga_soup\"],\n",
        "    \"shawarma\": [\"shawarma\", \"gyro\", \"arabic_wrap\", \"doner\", \"grilled_kebab\",\n",
        "                 \"souvlaki\", \"tawook\", \"shish_kebab\", \"kofta\", \"kibbeh\"],\n",
        "    \"seafood\": [\"seafood\", \"grilled_fish\", \"lobster\", \"shrimp_house\", \"oysters\",\n",
        "                \"sushi\", \"ceviche\", \"crab_cakes\", \"fish_tacos\", \"calamari\"],\n",
        "    \"vegetarian\": [\"vegetarian\", \"vegan\", \"plant_based\", \"organic_food\", \"tofu\",\n",
        "                   \"lentil_soup\", \"salad_bowl\", \"quinoa\", \"hummus\", \"avocado_toast\"],\n",
        "}\n",
        "\n",
        "\n",
        "# üîπ Function to get city coordinates\n",
        "def get_city_coordinates(city_name):\n",
        "    params = {\"address\": city_name, \"key\": API_KEY}\n",
        "    response = requests.get(GEOCODE_API_URL, params=params).json()\n",
        "\n",
        "    if response.get(\"status\") == \"OK\":\n",
        "        location = response[\"results\"][0][\"geometry\"][\"location\"]\n",
        "        return location[\"lat\"], location[\"lng\"]\n",
        "    else:\n",
        "        print(\"Error getting city coordinates:\", response)\n",
        "        return None, None\n",
        "\n",
        "# üîπ Function to get a list of restaurants from multiple grid locations\n",
        "def get_restaurants(city, total_required=2000, radius=1000):\n",
        "    lat, lng = get_city_coordinates(city)\n",
        "    if lat is None or lng is None:\n",
        "        return []\n",
        "\n",
        "    restaurants = []\n",
        "    headers = {\"X-Goog-Api-Key\": API_KEY, \"X-Goog-FieldMask\": \"places.displayName,places.id,places.rating,places.priceLevel,places.userRatingCount,places.location,places.types\"}\n",
        "\n",
        "    search_areas = [(lat + random.uniform(-0.1, 0.1), lng + random.uniform(-0.1, 0.1)) for _ in range(total_required // 20)]\n",
        "\n",
        "    for lat_grid, lng_grid in search_areas:\n",
        "        payload = {\n",
        "            \"includedTypes\": [\"restaurant\"],\n",
        "            \"locationRestriction\": {\n",
        "                \"circle\": {\n",
        "                    \"center\": {\"latitude\": lat_grid, \"longitude\": lng_grid},\n",
        "                    \"radius\": radius\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = requests.post(PLACES_SEARCH_URL, headers=headers, json=payload).json()\n",
        "        #print(response)\n",
        "\n",
        "        if \"places\" not in response:\n",
        "            print(\"Error fetching restaurant data:\", response)\n",
        "            continue\n",
        "\n",
        "        for place in response[\"places\"]:\n",
        "            restaurants.append({\n",
        "                \"place_id\": place[\"id\"],\n",
        "                \"name\": place[\"displayName\"][\"text\"],\n",
        "                \"rating\": place.get(\"rating\", 0),\n",
        "                \"total_ratings\": place.get(\"userRatingCount\", 0),\n",
        "                \"price_level\": place.get(\"priceLevel\", \"\"),\n",
        "                \"latitude\": place[\"location\"][\"latitude\"],\n",
        "                \"longitude\": place[\"location\"][\"longitude\"],\n",
        "                \"types\":place[\"types\"]\n",
        "            })\n",
        "\n",
        "            if len(restaurants) >= total_required:\n",
        "                return restaurants\n",
        "\n",
        "        time.sleep(1)  # Prevent API rate limits\n",
        "\n",
        "    return restaurants\n",
        "\n",
        "# üîπ Function to get additional restaurant details\n",
        "def get_restaurant_details_old(place_id):\n",
        "    headers = {\"X-Goog-Api-Key\": API_KEY, \"X-Goog-FieldMask\": \"displayName,formattedAddress,internationalPhoneNumber,regularOpeningHours,editorialSummary,reviews\"}\n",
        "    response = requests.get(f\"{PLACES_DETAILS_URL}{place_id}\", headers=headers).json()\n",
        "\n",
        "    if \"displayName\" not in response:\n",
        "        return {}\n",
        "\n",
        "    details = response\n",
        "    reviews = details.get(\"reviews\", [])\n",
        "\n",
        "    return {\n",
        "        \"phone_number\": details.get(\"internationalPhoneNumber\", \"\"),\n",
        "        \"full_address\": details.get(\"formattedAddress\", \"\"),\n",
        "        \"opening_hours\": details.get(\"regularOpeningHours\", {}).get(\"weekdayDescriptions\", []),\n",
        "        \"reviews\": [review[\"text\"][\"text\"] for review in reviews[:3]]  # Limit to top 3 reviews\n",
        "    }\n",
        "\n",
        "# üîπ Function to get additional restaurant details (Handles ALL missing cases)\n",
        "def get_restaurant_details(place_id):\n",
        "    headers = {\n",
        "        \"X-Goog-Api-Key\": API_KEY,\n",
        "        \"X-Goog-FieldMask\": \"displayName,formattedAddress,internationalPhoneNumber,regularOpeningHours,editorialSummary,reviews\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(f\"{PLACES_DETAILS_URL}{place_id}\", headers=headers).json()\n",
        "\n",
        "        if not response or \"displayName\" not in response:\n",
        "            return {\n",
        "                \"phone_number\": None,\n",
        "                \"full_address\": None,\n",
        "                \"opening_hours\": None,\n",
        "                \"reviews\": None\n",
        "            }\n",
        "\n",
        "        details = response\n",
        "\n",
        "        # ‚úÖ Handle missing phone number\n",
        "        phone_number = details.get(\"internationalPhoneNumber\", None)\n",
        "\n",
        "        # ‚úÖ Handle missing full address\n",
        "        full_address = details.get(\"formattedAddress\", None)\n",
        "\n",
        "        # ‚úÖ Handle missing opening hours (ensure proper fallback)\n",
        "        opening_hours = details.get(\"regularOpeningHours\", {}).get(\"weekdayDescriptions\", None)\n",
        "        if not opening_hours:\n",
        "            opening_hours = None  # Ensure consistent output if missing\n",
        "\n",
        "        # ‚úÖ Handle missing reviews (ensure list format)\n",
        "        reviews = details.get(\"reviews\", [])\n",
        "        if not isinstance(reviews, list):  # If it's not a list, make it an empty list\n",
        "            reviews = []\n",
        "\n",
        "        # ‚úÖ Extract up to 3 reviews safely (ensure no failure)\n",
        "        extracted_reviews = []\n",
        "        for review in reviews[:15]:\n",
        "            review_text = review.get(\"text\", {}).get(\"text\", None)\n",
        "            if review_text:  # Only add valid text\n",
        "                extracted_reviews.append(review_text)\n",
        "\n",
        "        return {\n",
        "            \"phone_number\": phone_number,\n",
        "            \"full_address\": full_address,\n",
        "            \"opening_hours\": opening_hours,\n",
        "            \"reviews\": extracted_reviews if extracted_reviews else None  # Ensure None if no reviews exist\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for place_id {place_id}: {str(e)}\")\n",
        "        return {\n",
        "            \"phone_number\": None,\n",
        "            \"full_address\": None,\n",
        "            \"opening_hours\": None,\n",
        "            \"reviews\": None\n",
        "        }\n",
        "\n",
        "def extract_cuisine_from_text(text):\n",
        "    \"\"\"\n",
        "    Finds relevant cuisine types from a given text (reviews, restaurant types).\n",
        "    Matches both exact cuisine names and their variants.\n",
        "    \"\"\"\n",
        "    text = text.lower().replace(\"-\", \"_\")  # Normalize text\n",
        "    matched_cuisines = set()\n",
        "\n",
        "    for cuisine, keywords in CUISINE_KEYWORDS.items():\n",
        "        if any(re.search(rf\"\\b{kw}\\b\", text) for kw in keywords):\n",
        "            matched_cuisines.add(cuisine)\n",
        "\n",
        "    return list(matched_cuisines) if matched_cuisines else [\"unknown\"]\n",
        "\n",
        "\n",
        "# üîπ Function to collect and save data for multiple cities\n",
        "def collect_and_save_data(cities, total_required_per_city=2000, output_file=\"restaurants_data_v2.csv\"):\n",
        "    all_restaurants = []  # Store data for all cities\n",
        "\n",
        "    for city in cities:\n",
        "        print(f\"üåç Collecting data for {city}...\")\n",
        "        restaurants = get_restaurants(city, total_required=total_required_per_city)\n",
        "\n",
        "        for i, restaurant in enumerate(restaurants):\n",
        "            details = get_restaurant_details(restaurant[\"place_id\"])\n",
        "            restaurant.update(details)\n",
        "            all_restaurants.append(restaurant)\n",
        "\n",
        "            if i % 50 == 0:  # Save progress every 50 records\n",
        "                df = pd.DataFrame(all_restaurants)\n",
        "                df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "                #print(f\"‚úÖ {len(all_restaurants)} total restaurants collected so far...\")\n",
        "\n",
        "    # Final save after all cities are processed\n",
        "    df = pd.DataFrame(all_restaurants)\n",
        "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"‚úÖ Data collection complete: {len(df)} restaurants saved from {len(cities)} cities.\")\n",
        "\n",
        "# üîπ Define the list of cities to collect data from\n",
        "cities_list = [\"Makkah, Saudi Arabia\", \"Madinah, Saudi Arabia\"]\n",
        "\n",
        "# üîπ Run the script for multiple cities\n",
        "#collect_and_save_data(cities_list, total_required_per_city=5000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSHPe6mClbhf",
        "outputId": "bc902fe4-8812-440f-8e6b-b83f01ac0d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cleaning complete! 3014 unique restaurants saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-3d41ac7ae531>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  remaining_best = remaining_duplicates.groupby(\"place_id\").apply(lambda x: x.loc[x.isnull().sum(axis=1).idxmin()])\n",
            "<ipython-input-10-3d41ac7ae531>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_duplicates[col] = filtered_duplicates[col].astype(str)\n",
            "<ipython-input-10-3d41ac7ae531>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_duplicates[col] = filtered_duplicates[col].astype(str)\n",
            "<ipython-input-10-3d41ac7ae531>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_duplicates[col] = filtered_duplicates[col].astype(str)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the dataset\n",
        "file_path = \"restaurants_data_v2.csv\"  # Update with your file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the columns that must be non-null in the best selection\n",
        "required_columns = [\"place_id\", \"name\", \"rating\", \"total_ratings\", \"price_level\",\n",
        "                    \"latitude\", \"longitude\", \"opening_hours\", \"reviews\", \"cuisine\"]\n",
        "\n",
        "df[\"cuisine\"] = (df[\"types\"].astype(str) + \" \" + df[\"reviews\"].astype(str)).apply(lambda row: extract_cuisine_from_text(row))  # Convert string to list\n",
        "\n",
        "# Step 1: Identify duplicate restaurants based on `place_id`\n",
        "duplicates = df[df.duplicated(subset=[\"place_id\"], keep=False)]  # Find all duplicate restaurants\n",
        "\n",
        "# Step 2: Select the duplicate entries that have **all required columns filled**\n",
        "filtered_duplicates = duplicates.dropna(subset=required_columns)\n",
        "\n",
        "# Step 3: If a restaurant has **no fully complete duplicates**, pick one with the least missing values\n",
        "remaining_duplicates = duplicates[~duplicates[\"place_id\"].isin(filtered_duplicates[\"place_id\"])]\n",
        "remaining_best = remaining_duplicates.groupby(\"place_id\").apply(lambda x: x.loc[x.isnull().sum(axis=1).idxmin()])\n",
        "\n",
        "# Identify list-type columns and convert them to strings\n",
        "list_columns = [\"cuisine\", \"reviews\", \"types\"]  # Adjust based on actual data\n",
        "\n",
        "for col in list_columns:\n",
        "    if col in remaining_best.columns:\n",
        "        remaining_best[col] = remaining_best[col].astype(str)\n",
        "        filtered_duplicates[col] = filtered_duplicates[col].astype(str)\n",
        "\n",
        "remaining_best = remaining_best.reset_index(drop=True)\n",
        "\n",
        "final_selection = pd.concat([filtered_duplicates, remaining_best]).drop_duplicates()\n",
        "\n",
        "# Step 5: Remove all other duplicate entries from the original dataset\n",
        "df_cleaned = df[~df[\"place_id\"].isin(duplicates[\"place_id\"])]  # Keep only unique restaurants\n",
        "df_cleaned = pd.concat([df_cleaned, final_selection])  # Add back the best duplicates\n",
        "\n",
        "# Save the cleaned dataset\n",
        "cleaned_file_path = \"restaurants_cleaned.csv\"\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Cleaning complete! {len(df_cleaned)} unique restaurants saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG4KjbD2lhN7",
        "outputId": "3e81f747-bfe4-407e-c226-ebea9272a51d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Feature Engineering Complete. Processed dataset saved as `restaurants_data_processed.csv`.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from geopy.distance import geodesic\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download sentiment analysis model (only once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"restaurants_cleaned.csv\"  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "### üîπ 1Ô∏è‚É£ Normalize `rating`\n",
        "df[\"normalized_rating\"] = df[\"rating\"] / 5.0  # Convert to 0-1 scale\n",
        "\n",
        "### üîπ 2Ô∏è‚É£ Compute `Weighted Rating` (rating * total reviews)\n",
        "df[\"weighted_rating\"] = df[\"rating\"] * (df[\"total_ratings\"] / df[\"total_ratings\"].max())\n",
        "\n",
        "### üîπ 3Ô∏è‚É£ Convert `price_level` to Numeric\n",
        "price_mapping = {\n",
        "    \"PRICE_LEVEL_INEXPENSIVE\": 1,\n",
        "    \"PRICE_LEVEL_MODERATE\": 2,\n",
        "    \"PRICE_LEVEL_EXPENSIVE\": 3\n",
        "}\n",
        "df[\"price_level_numeric\"] = df[\"price_level\"].map(price_mapping).fillna(0)  # Fill NaN with 0\n",
        "\n",
        "### üîπ 6Ô∏è‚É£ Sentiment Analysis on `reviews`\n",
        "def get_sentiment_score(reviews):\n",
        "    \"\"\"Compute average sentiment score from reviews\"\"\"\n",
        "    if pd.isna(reviews):\n",
        "        return 0  # Neutral sentiment if no reviews\n",
        "    try:\n",
        "        review_list = ast.literal_eval(reviews)  # Convert string to list\n",
        "        scores = [sia.polarity_scores(text)[\"compound\"] for text in review_list if text]\n",
        "        return np.mean(scores) if scores else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "df[\"sentiment_score\"] = df[\"reviews\"].apply(get_sentiment_score)\n",
        "\n",
        "# Perform One-Hot Encoding\n",
        "df[\"cuisine_types\"] = df[\"cuisine\"]\n",
        "df = pd.get_dummies(df, columns=[\"cuisine_types\"], prefix=\"cuisine\", dtype=float)\n",
        "\n",
        "### üîπ 7Ô∏è‚É£ Drop Unnecessary Columns\n",
        "df.drop(columns=[\"place_id\", \"phone_number\", \"full_address\", \"opening_hours\", \"reviews\",\"types\"], inplace=True)\n",
        "\n",
        "# Save the processed dataset\n",
        "df.to_csv(\"restaurants_data_processed.csv\", index=False,encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"‚úÖ Feature Engineering Complete. Processed dataset saved as `restaurants_data_processed.csv`.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z3jwYP9lgvM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67wr7dy4llEc",
        "outputId": "3ffa5e0f-3c6c-4912-dbb2-8d66c7541a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data Preprocessing Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-c246e9a24be3>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"rating\"].fillna(df[\"rating\"].mean(), inplace=True)  # Fill NaN ratings with average\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load Processed Data\n",
        "file_path = \"restaurants_data_processed.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Fill missing values\n",
        "df[\"rating\"].fillna(df[\"rating\"].mean(), inplace=True)  # Fill NaN ratings with average\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Create a \"fake user ID\" based on restaurant reviews\n",
        "df[\"user_id\"] = df.index  # Temporary user ID\n",
        "\n",
        "# Normalize numerical features (Rating, Distance, Sentiment Score, etc.)\n",
        "# Identify numerical columns\n",
        "numerical_features = [\"normalized_rating\", \"weighted_rating\", \"price_level_numeric\", \"sentiment_score\"]\n",
        "\n",
        "# Find all one-hot encoded cuisine columns (those starting with \"cuisine_\")\n",
        "cuisine_features = [col for col in df.columns if col.startswith(\"cuisine_\")]\n",
        "\n",
        "# Combine numerical and cuisine features\n",
        "features_to_normalize = numerical_features + cuisine_features\n",
        "\n",
        "# Apply Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n",
        "\n",
        "print(\"‚úÖ Data Preprocessing Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHUHna3jlrCf",
        "outputId": "2a3e4338-8ab0-4bd5-c47d-46707cff08ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-1e5b3d794d9f>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"preference_score\"] = df[\"normalized_rating\"]  # Placeholder labels\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ML-Based Recommendations Using Random Forest Ready!\n",
            "                                         name  predicted_score      cuisine\n",
            "913                              ŸÖÿ≥ÿ™ŸàÿØÿπ ÿßÿ´ŸäŸÑŸá              1.0  ['unknown']\n",
            "2947                          ŸÅŸàÿØ ÿ™ÿ±ÿßŸÉ ÿ≥ŸÜŸÅŸàÿ±Ÿá              1.0  ['unknown']\n",
            "2073                           ŸÖÿ∑ÿπŸÖ ÿ£ÿ±Ÿäÿ¨ ÿØŸÖÿ¥ŸÇ              1.0   ['arabic']\n",
            "528   ŸÖÿ∑ÿ®ÿÆ ÿ¥ÿ±ŸÉÿ© ÿßŸÑÿ≠ÿ¨ÿßÿ≤ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑÿßÿπÿßÿ¥ÿ©              1.0  ['unknown']\n",
            "2742                            ÿ¥ÿπÿ®Ÿäÿßÿ™ ÿßŸÑÿßÿµŸäŸÑ              1.0  ['unknown']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-1e5b3d794d9f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"predicted_score\"] = rf_model.predict(df[features_to_normalize])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy Preference Scores (If No User Data, Use Avg Ratings as Labels)\n",
        "df[\"preference_score\"] = df[\"normalized_rating\"]  # Placeholder labels\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features_to_normalize], df[\"preference_score\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict Scores for All Restaurants\n",
        "df[\"predicted_score\"] = rf_model.predict(df[features_to_normalize])\n",
        "\n",
        "# Recommend Top Restaurants\n",
        "def recommend_restaurants_rf(top_n=5):\n",
        "    return df.sort_values(by=\"predicted_score\", ascending=False)[[\"name\", \"predicted_score\", \"cuisine\"]].head(top_n)\n",
        "\n",
        "# Example Usage\n",
        "rf_recommendations = recommend_restaurants_rf(top_n=5)\n",
        "\n",
        "print(\"‚úÖ ML-Based Recommendations Using Random Forest Ready!\")\n",
        "\n",
        "print(rf_recommendations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqM3EDesl3gn",
        "outputId": "ed41400f-c7f7-41de-ddb3-d58d4e7f876c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Random Forest Evaluation:\n",
            "MSE: 0.0000, RMSE: 0.0026, MAE: 0.0002\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Predict ratings on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate Errors\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Random Forest Evaluation:\\nMSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wQjd9Ep0pGyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "### üîπ 4Ô∏è‚É£ Compute `Distance from User`\n",
        "# Define a sample user location (Makkah City Center)\n",
        "\n",
        "\n",
        "def compute_distance(lat, lon, user_location):\n",
        "    \"\"\"Compute distance between restaurant and user\"\"\"\n",
        "    try:\n",
        "        return geodesic(user_location, (lat, lon)).km\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Function to get distance from Google Maps API v2\n",
        "def get_google_distance_v2(user_lat, user_lon, rest_lat, rest_lon, travel_mode=\"driving\"):\n",
        "\n",
        "    \"\"\" Get real-world distance using Google API v2 \"\"\"\n",
        "\n",
        "    url = f\"https://routes.googleapis.com/directions/v2:computeRoutes\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-Goog-Api-Key\": API_KEY,\n",
        "        \"X-Goog-FieldMask\": (\n",
        "            \"routes.distanceMeters,\"\n",
        "            \"routes.duration,\"\n",
        "            \"routes.legs.steps.navigationInstruction,\"\n",
        "            \"routes.legs.distanceMeters,\"\n",
        "            \"routes.legs.duration,\"\n",
        "            \"routes.legs.startLocation,\"\n",
        "            \"routes.legs.endLocation\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"origin\": {\"location\": {\"latLng\": {\"latitude\": user_lat, \"longitude\": user_lon}}},\n",
        "        \"destination\": {\"location\": {\"latLng\": {\"latitude\": rest_lat, \"longitude\": rest_lon}}},\n",
        "        \"travelMode\": travel_mode.upper()  # \"DRIVING\", \"WALKING\", \"BICYCLING\", \"TRANSIT\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if \"routes\" in data and len(data[\"routes\"]) > 0:\n",
        "            #return data[\"routes\"][0][\"distanceMeters\"] / 1000  # Convert meters to km\n",
        "            #return data[\"routes\"]\n",
        "\n",
        "            distance = data[\"routes\"][0][\"distanceMeters\"] / 1000  # Convert meters to km\n",
        "            duration_sec = int(data[\"routes\"][0][\"duration\"].replace(\"s\", \"\"))  # Convert \"441s\" ‚Üí 441\n",
        "            duration_min = round(duration_sec / 60, 1)  # Convert seconds to minutes\n",
        "\n",
        "            return pd.Series([distance, duration_min])\n",
        "\n",
        "    return pd.Series([None, None])  # Return None if no distance found\n",
        "\n",
        "\n",
        "def get_distance_and_duration(location,  lat2,lon2,  mode=\"driving\"):\n",
        "    \"\"\"\n",
        "    Fetch distance and duration between two coordinates using OSRM API.\n",
        "\n",
        "    Parameters:\n",
        "        lat1, lon1: Latitude & Longitude of Origin\n",
        "        lat2, lon2: Latitude & Longitude of Destination\n",
        "        mode: Transport mode (\"driving\", \"walking\", \"cycling\")\n",
        "\n",
        "    Returns:\n",
        "        Distance (km), Duration (minutes)\n",
        "    \"\"\"\n",
        "    lat1, lon1 = location\n",
        "\n",
        "    #print(f\"Requesting: {lon1}, {lat1} to {lon2}, {lat2} in mode: {mode}\")  # Debugging statement\n",
        "\n",
        "\n",
        "    url = f\"https://router.project-osrm.org/route/v1/{mode}/{lon1},{lat1};{lon2},{lat2}?overview=false\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"routes\" in data and data[\"routes\"]:\n",
        "        distance = data[\"routes\"][0][\"distance\"] / 1000  # Convert meters to kilometers\n",
        "        duration = data[\"routes\"][0][\"duration\"] / 60  # Convert seconds to minutes\n",
        "        #print(distance, duration)\n",
        "        return pd.Series ([round(distance, 2), round(duration, 2)])\n",
        "    else:\n",
        "        return  pd.Series ([None, None])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LSkJemRrj6Cl"
      },
      "outputs": [],
      "source": [
        "# Assuming you already have the trained model\n",
        "#from your_model_file import model, preprocess_input\n",
        "\n",
        "def preprocess_input (user_input):\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    location = user_input['location']\n",
        "\n",
        "    if 'cuisine' in user_input:\n",
        "        print(f\"Cuisine: {user_input['cuisine']}\")\n",
        "        df_filtered = df_filtered[df_filtered[\"cuisine\"].str.contains(user_input['cuisine'].lower(), case=False, na=False)]\n",
        "\n",
        "    if 'rating' in user_input:\n",
        "        print(f\"Rating: {user_input['rating']}\")\n",
        "        df_filtered = df_filtered[df_filtered[\"normalized_rating\"]*5 > float(user_input['rating'])]\n",
        "\n",
        "    if 'price_range' in user_input:\n",
        "        print(f\"Price Range: {user_input['price_range']}\")\n",
        "        df_filtered = df_filtered[df_filtered[\"price_level_numeric\"] <= float(user_input['price_range'])]\n",
        "\n",
        "    if 'distance' in user_input:\n",
        "        print(f\"Distance: {user_input['distance']}\")\n",
        "        distance = user_input['distance']\n",
        "\n",
        "    if 'top_n' in user_input:\n",
        "        print(f\"top_n: {user_input['top_n']}\")\n",
        "        top_n = user_input['top_n']\n",
        "    else:\n",
        "        top_n = 10\n",
        "\n",
        "    if 'location' in user_input:\n",
        "        print(f\"Location: {user_input['location']}\")\n",
        "        location = user_input['location']\n",
        "    else:\n",
        "        location = (21.4225, 39.8262)\n",
        "\n",
        "    #print(df_filtered.empty)\n",
        "    if not(df_filtered.empty):\n",
        "        # Apply to DataFrame (assuming df has 'latitude' & 'longitude' columns)\n",
        "        df_filtered[[\"distance_in_kms\",\"duration_in_mins\"]] = df_filtered.apply(lambda row: get_distance_and_duration(location, row[\"latitude\"], row[\"longitude\"], mode=\"driving\"), axis=1)\n",
        "        df_filtered = df_filtered[df_filtered[\"distance_in_kms\"] <= float(distance)]\n",
        "        print(f\"‚úÖ Found {len(df_filtered)} restaurants serving {user_input['cuisine']} food.\")\n",
        "\n",
        "        # Ensure Random Forest Model is already trained\n",
        "        df_filtered[\"predicted_score\"] = rf_model.predict(df_filtered[features_to_normalize])\n",
        "\n",
        "        # Sort by highest predicted preference score\n",
        "        recommended_restaurants = df_filtered.sort_values(by=\"predicted_score\", ascending=False)[[\"name\", \"normalized_rating\", \"predicted_score\", \"distance_in_kms\",\"duration_in_mins\", \"cuisine\",\"longitude\",\"latitude\"]].head(10)\n",
        "        recommended_restaurants[\"normalized_rating\"] = recommended_restaurants[\"normalized_rating\"] * 5\n",
        "        # Get the top n rows with highest predicted_score\n",
        "        recommended_restaurants = recommended_restaurants.nlargest(int(top_n), 'predicted_score')\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No Restaurant found with the requested parameters.\")\n",
        "        return None\n",
        "\n",
        "    return recommended_restaurants\n",
        "\n",
        "def recommend_restaurants(cuisine=None, rating=None, price_range=None, distance=None, location=None, top_n=None):\n",
        "    # Initialize a dictionary to hold the input values\n",
        "    user_input = {}\n",
        "\n",
        "    # Check if the input was provided for each field, and add it to the input dictionary\n",
        "    if cuisine:  # If cuisine is provided\n",
        "        user_input['cuisine'] = cuisine\n",
        "\n",
        "    if rating:  # If rating is provided\n",
        "        user_input['rating'] = rating\n",
        "\n",
        "    if price_range:  # If price range is provided\n",
        "        user_input['price_range'] = price_range\n",
        "\n",
        "    if distance:  # If distance is provided\n",
        "        user_input['distance'] = distance\n",
        "\n",
        "    if location:  # If location is provided\n",
        "        user_input['location'] = location\n",
        "\n",
        "    if top_n:  # If location is provided\n",
        "        user_input['top_n'] = top_n\n",
        "\n",
        "    # Now you can preprocess the user_input before feeding it into the model\n",
        "    # For example, you might need to adjust preprocessing depending on which fields were filled\n",
        "\n",
        "    recommended_restaurants = preprocess_input(user_input)\n",
        "\n",
        "    return recommended_restaurants\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s87C8G52Wt4n",
        "outputId": "61915992-647e-4bfe-cbf1-e52c1f16a6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://4bea-104-196-201-225.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:17:46] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:17:46] \"GET /ngrok_url.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:17:47] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:19:05] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:19:05] \"\u001b[36mGET /ngrok_url.js HTTP/1.1\u001b[0m\" 304 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuisine: lebanese\n",
            "Rating: 4\n",
            "Price Range: 3\n",
            "Distance: 10\n",
            "top_n: 5\n",
            "Location: ('21.4225', '39.8262')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [16/Mar/2025 23:19:25] \"POST /recommend HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 2 restaurants serving Pakistani food.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Allows cross-origin requests\n",
        "\n",
        "def clean_cuisine(text):\n",
        "    text = re.sub(r\"[\\[\\]']\", \"\", text)  # Remove brackets and quotes\n",
        "    words = text.split(\", \")  # Split into individual words\n",
        "    return \", \".join(word.strip().capitalize() for word in words)  # Convert to Camel Case\n",
        "\n",
        "\n",
        "\n",
        "# Expose the HTML page\n",
        "@app.route(\"/\")\n",
        "def serve_html():\n",
        "    return send_file(\"/content/restaurant_recommendation_dynamic.html\")\n",
        "\n",
        "\n",
        "@app.route(\"/ngrok_url.js\")\n",
        "def serve_js():\n",
        "    return send_file(\"/content/ngrok_url.js\")\n",
        "\n",
        "# Flask Route to Handle Requests\n",
        "@app.route(\"/recommend\", methods=[\"POST\"])\n",
        "def recommend():\n",
        "    data = request.json  # Get JSON request data\n",
        "    cuisine = data.get(\"cuisine\", \"pakistani\")\n",
        "    min_rating = data.get(\"min_rating\", 3.5)\n",
        "    price_range = data.get(\"price_range\", 3)\n",
        "    max_distance = data.get(\"max_distance\", 10)\n",
        "    latitude = data.get(\"latitude\", 21.4225)\n",
        "    longitude = data.get(\"longitude\", 39.8262)\n",
        "    top_n = data.get(\"top_n\", 5)\n",
        "\n",
        "    recommendations = recommend_restaurants(cuisine, min_rating,price_range, max_distance, (latitude, longitude), top_n)\n",
        "    recommendations[\"cuisine\"] = recommendations[\"cuisine\"].apply(clean_cuisine)  # Apply to each row\n",
        "\n",
        "    return jsonify(recommendations.to_dict(orient=\"records\"))  # Return results as JSON\n",
        "\n",
        "# Start the server using ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "\n",
        "# Save the URL for frontend use\n",
        "with open(\"/content/ngrok_url.js\", \"w\") as f:\n",
        "    f.write(f'const NGROK_URL = \"{public_url}\";')\n",
        "\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTysjGXnXIgM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://9c69-104-196-201-225.ngrok-free.app/recommend\"\n",
        "data = {\n",
        "    \"cuisine\": \"Pakistani\",\n",
        "    \"min_rating\": 3.5,\n",
        "    \"max_distance\": 10,\n",
        "    \"latitude\": 21.4225,\n",
        "    \"longitude\": 39.8262,\n",
        "    \"top_n\": 5\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.status_code)  # Should be 200 if working\n",
        "print(response.json())  # Expected: JSON list of recommended restaurants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AXUTO892xOy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
